import enum
from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any, Optional

import torch

from vllm.logger import init_logger
from vllm.v1.core.sched.output import SchedulerOutput

from vllm.sequence import IntermediateTensors
from vllm.distributed.parallel_state import (
    get_world_group,
    get_tp_group,
    get_pp_group,
)
from .af_connector import AFConnectorBase, AFConnectorRole, AFConnectorMetadata

if TYPE_CHECKING:
    from vllm.attention.backends.abstract import AttentionMetadata
    from vllm.config import VllmConfig
    from vllm.forward_context import ForwardContext
    from vllm.v1.core.kv_cache_manager import KVCacheBlocks
    from vllm.v1.request import Request

logger = init_logger(__name__)


class NaiveConnectorMetadata:
    """
    Abstract Metadata used to communicate between scheduler side and worker
    side Connector
    """

    pass


class NaiveConnector(AFConnectorBase):
    """
    Assuming no DP and no PP
    """

    def __init__(self, vllm_config: "VllmConfig", role: AFConnectorRole):
        super().__init__(vllm_config, role)
        logger.warning(
            "Initializing AFConnectorBase. This API is experimental and "
            "subject to change in the future as we iterate the design."
        )
        self._connector_metadata = AFConnectorMetadata()
        self._vllm_config = vllm_config
        self._role = role

        # build the sender/receiver table
        self.a2f_rank, self.f2a_rank = self._build_sender_receiver_table()
        self.afd_rank = get_world_group().rank

    def bind_metadata(self, metadata: AFConnectorMetadata):
        self._connector_metadata = metadata

    # -------------------------------------------------------------------------
    #                                attn -> ffn
    # -------------------------------------------------------------------------
    def a2f_attn(self, intermediate_tensors: IntermediateTensors):
        """
        This method will be called by the ATTN side.

        * To send the intermediate tensors generated by ATTN instances to FFN.
        """
        # 1. decide which one should be sent to
        assert (
            "hidden_states" in intermediate_tensors
            and "residual" in intermediate_tensors
        ), (
            "should contains both hidden states and residual when "
            "transfer from Attention to FFN in NaiveConnector"
        )

        # 2. synchronized send method is called,
        # TODO: should use async version
        if not self._is_tp_head():
            logger.info(
                "tp rank %s doesn't participate in the ATTN to FFN transfer",
                get_tp_group().rank_in_group,
            )
            return

        get_world_group().send_tensor_dict(
            intermediate_tensors.tensors,
            dst=self.a2f_rank[self.afd_rank],
            all_gather_group=None,  # don't slice and send, just send for now
        )

    def a2f_ffn(self) -> IntermediateTensors:
        """
        This method will be called by the FFN side.

        * To receive the intermediate tensors from ATTN.
        * And (Maybe) dispatch them from the receiver to other GPUs.
        """
        intermediate_tensors = get_world_group().recv_tensor_dict(
            src=self.f2a_rank[self.afd_rank],
            all_gather_group=None,
        )
        return IntermediateTensors(intermediate_tensors)

    # -------------------------------------------------------------------------
    #                                attn <- ffn
    # -------------------------------------------------------------------------
    def f2a_ffn(self, intermediate_tensors: IntermediateTensors):
        """
        This method will be called by the FFN side.

        * To send the intermediate tensors generated by FFN instances back to
            the sender (this should be the same GPU as it comes from)
        """
        get_world_group().send_tensor_dict(
            intermediate_tensors.tensors,
            dst=self.f2a_rank[self.afd_rank],
        )

    def f2a_attn(self, intermediate_tensors: IntermediateTensors):
        """
        This method will be called by the ATTN side.

        * To receive the MOE output intermediate tensors.
        * And (Maybe) dispatch them from the receiver to other GPUs.
            (this should be the same GPU as it comes from)
        """
        intermediate_tensors = get_world_group().recv_tensor_dict(
            src=self.f2a_rank[self.afd_rank],
            all_gather_group=None,
        )
        return intermediate_tensors

    # -------------- OTHER HELPERS ------------------
    def _is_tp_head(self) -> bool:
        return not (
            self._vllm_config.parallel_config.tensor_parallel_size > 1
            and get_tp_group().rank_in_group != 0
        )

    def _build_sender_receiver_table(
        self,
    ) -> tuple[dict[int, int], dict[int, int]]:
        """
        Assuming:
                            DP = 2
        Model parallel   :  TP = 8, no PP
        Disaggregated FFN : FFN = 8

        Then it could be like:
        DP-0 [  0,  1,  2,  3,  4,  5,  6,  7, ] ==> tp (for attn)
                |   |   |   |   |   |   |   |    ==>    a2f/f2a mapping
             [  8,  9, 10, 11, 12, 13, 14, 15, ] ==> ffn (for ffn)

        DP-2 [ 16, 17, 18, 19, 20, 21, 22, 23, ] ==> tp (for attn)
                |   |   |   |   |   |   |   |    ==>    a2f/f2a mapping
             [ 24, 25, 26, 27, 28, 29, 30, 31  ] ==> ffn (for ffn)

        If less ffn, then try to map evenly as possible
        [  0,  1,  2,  3,  4,  5,  6,  7, ]    ==> tp (for attn)
            \ /     \ /     \ /     \ /        ==> a2f/f2a mapping
        [    8,      9,      10,     11, ... ] ==> ffn (for ffn)

        Returns:
            [ { src : dst } , { dst : src } ]
        """
        parallel_config = self._vllm_config.parallel_config

        # only tp 0 will send something,
        sender_cnt = parallel_config.data_parallel_size

        # if larger than tp, then it's ffn
        ffn_ranks = [
            i
            for i in range(parallel_config.world_size)
            if (i % parallel_config.world_size)
            > parallel_config.tensor_parallel_size
        ]

        a2f = {
            s * parallel_config.tensor_parallel_size: s % len(ffn_ranks)
            for s in range(sender_cnt)
        }
        return a2f, {f: a for a, f in a2f.items()}
